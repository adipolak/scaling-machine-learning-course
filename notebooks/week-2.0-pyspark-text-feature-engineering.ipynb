{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f2d36c",
   "metadata": {},
   "source": [
    "# ⭐ Scaling Machine Learning in Three Week course - Week 2:\n",
    "##  PySpark - text - feature engineering\n",
    "\n",
    "In this excercise, you will use:\n",
    " * Bot data set\n",
    " * Work with Null values\n",
    " * \n",
    "\n",
    "\n",
    "\n",
    "This excercise is part of the [Scaling Machine Learning with Spark book](https://learning.oreilly.com/library/view/scaling-machine-learning/9781098106812/)\n",
    "available on the O'Reilly platform or on [Amazon](https://amzn.to/3WgHQvd).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Scalling_ml_with_spark-week_2') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03dfcd",
   "metadata": {},
   "source": [
    "## ✅ **Task  zzz :** Tokanizer -> N Gram\n",
    "\n",
    "Extract an NGram using Tokanizer and connect data marshaling with Feature engineering.\n",
    "For that, use `NGram` functionality.\n",
    "\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "    \n",
    "Checkout functionalities like fillna:\n",
    "    \n",
    "```python\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "wordDataFrame = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)\n",
    "    \n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc10f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark \"),\n",
    "    (1, \"I wish, wish Java, Java could\"),\n",
    "    (2, \"Logistic regression, regression models\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "# your solution goes here\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0eef96",
   "metadata": {},
   "source": [
    "## ✅ **Task ZZZ :**  PCA\n",
    "\n",
    "Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation\n",
    "\n",
    "Take the DF and reduce the demensions of the vectores using `PCA`.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary> Click here to see the Solution</summary>\n",
    "<p>\n",
    "    \n",
    "here:\n",
    "    \n",
    "```python\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)\n",
    "```\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa857bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# your solution goes here\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbf56d",
   "metadata": {},
   "source": [
    "## ✅ **Task :**  check for null values\n",
    "how many null values are in the data?\n",
    "\n",
    "How do you validate and overcome null values in your data?\n",
    "\n",
    "Take a look at the slide we discussed before and decide how to operate on the null values.\n",
    "\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "    \n",
    "Checkout functionalities like fillna:\n",
    "    \n",
    "```python\n",
    "df.fillna({'bot':0})\n",
    "    \n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232f8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58045ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566e5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
