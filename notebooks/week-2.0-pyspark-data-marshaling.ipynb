{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5236c64",
   "metadata": {},
   "source": [
    "# ‚≠ê Scaling Machine Learning in Three Week course - Week 2:\n",
    "##  PySpark - data marshaling\n",
    "\n",
    "In this excercise, you will use:\n",
    " * Mock data\n",
    " * Bot data set\n",
    " * DataFrame\n",
    " * Spark SQL\n",
    " * Spark Summary\n",
    "\n",
    "\n",
    "\n",
    "This excercise is part of the [Scaling Machine Learning with Spark book](https://learning.oreilly.com/library/view/scaling-machine-learning/9781098106812/)\n",
    "available on the O'Reilly platform or on [Amazon](https://amzn.to/3WgHQvd).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf9321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Scalling_ml_with_spark-week_2') \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0655910",
   "metadata": {},
   "source": [
    "## ‚úÖ **Task 1 :** Split text\n",
    "\n",
    "How would you go about taking text and turning it into words using PySpark functionality?\n",
    "\n",
    "Have a look at `Tokenizer` and `RegexTokenizer`.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary> Click here to see the Solution </summary>\n",
    "<p>\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b4b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi|I|heard|about|Spark\"),\n",
    "    (1, \"I     wish Java      could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "\n",
    "# your solutions goes here \n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee57bf5",
   "metadata": {},
   "source": [
    "## ‚úÖ **Task 2:** words into numerical vectors\n",
    "\n",
    "How can you take a string of text and turn it into a numerical vectors representation of words? Spark has a unique functionality for that - `Word2Vec`.\n",
    "\n",
    "Take the documentDF and use `Word2Vec` to turn the string of text into a numerical vectors.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary> Click here to see the Solution</summary>\n",
    "<p>\n",
    "    \n",
    "\n",
    "    \n",
    "```python\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n",
    "    \n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Your answer goes here ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60ba2e",
   "metadata": {},
   "source": [
    "## ‚úÖ **Task 3:** stop words\n",
    "\n",
    "How can you take a document of words and remove the noisy stop words?\n",
    "\n",
    "Take the documentDF and use `StopWordsRemover` to turn the string of text into a numerical vectors.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary> Click here to see the Solution</summary>\n",
    "<p>\n",
    "    \n",
    "here:\n",
    "    \n",
    "```python\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show(truncate=False)\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "\n",
    "# your solution goes here\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15e8c5",
   "metadata": {},
   "source": [
    "## ‚úÖ **Task 4:** Binarized features\n",
    "\n",
    "Sometimes, vectorized features, need to go through a binarization process to better fit the problem domain.\n",
    "\n",
    "Take a continues column and use `Binarizer` to transform the feature into a binary one.\n",
    "\n",
    "<br>\n",
    "\n",
    "<details><summary> Click here to see the Solution</summary>\n",
    "<p>\n",
    "    \n",
    "here:\n",
    "    \n",
    "```python\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 5.1),\n",
    "    (1, 5.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "# your solution goes here\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acf43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b94299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1391eec0",
   "metadata": {},
   "source": [
    "### ‚úÖ **Task 5 :**  load the bot data & marshell it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv ('../datasets/bot_data.csv', header= True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f3928",
   "metadata": {},
   "source": [
    "#### get a reminder of how this data looks like:\n",
    "\n",
    "Look at 2 records from the DataFrame to understand the values better before filter: use take() function\n",
    "\n",
    "df.take(insert an integer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0271ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9da47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(25) .toPandas ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e23d4",
   "metadata": {},
   "source": [
    "How many lines have missing values? run the next command to figure it out!\n",
    "\n",
    "```python\n",
    "import pyspark.sql.functions as f\n",
    "from functools import reduce\n",
    "df.where(reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))).count()\n",
    "functools is a python 3 library.\n",
    "```\n",
    "\n",
    "reduce is part of functools, it takes two arguments: x and y, and produce cumulative items of iterable - in our case: x | y | is python OR operator, we concat x and y functionality with OR operator\n",
    "\n",
    "For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5)\n",
    "\n",
    "Run only reduce function and check the output:\n",
    "\n",
    "```reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d26163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c1088",
   "metadata": {},
   "source": [
    "You created a concatenation of OR operators with IS NULL functionality for all the columns!\n",
    "\n",
    "Now, put it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from functools import reduce\n",
    "\n",
    "reducePhrase = reduce(lambda x, y: x | y, (f.col(x).isNull() for x in df.columns))\n",
    "\n",
    "df.where(reducePhrase).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa3bdf",
   "metadata": {},
   "source": [
    "#### Distinct Value\n",
    "Get the sum of id distinct values, it should be equal to the size of the data\n",
    "\n",
    "Try both id and id_str fields.\n",
    "\n",
    "Use the next code and adjust it according to the field:\n",
    "\n",
    "```python\n",
    "df.select(\"field_name\").distinct().count()\n",
    "```\n",
    "What happened here? Is it in the same size of the data set? Don't worry; We fix that soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a696e",
   "metadata": {},
   "source": [
    "#### Is Null\n",
    "\n",
    "How many rows have null on the screen_name column?\n",
    "\n",
    "Use the where with col .isNull function to get the DataFrame with null value for column_name.\n",
    "\n",
    "Count it! Use the count method for that.\n",
    "\n",
    "Code sample:\n",
    "\n",
    "```python\n",
    "df.where(f.col('column_name').isNull()).count()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85547d83",
   "metadata": {},
   "source": [
    "#### Standard Deviation\n",
    "As part of exploring the data phase, the standard deviation(stddev) is a must!\n",
    "\n",
    "Calculate stddev for followers_count.\n",
    "\n",
    "#### Notice!\n",
    "Some rows have None/Null for followers_count, we can:\n",
    "\n",
    "Ignore and not calculate the stddev for them\n",
    "#### OR\n",
    "\n",
    "Give them a default value\n",
    "#### OR\n",
    "\n",
    "Filter them entirely out of our training data.\n",
    "Start with counting how many rows has null for followers_count:\n",
    "\n",
    "Run this:\n",
    "```python\n",
    "df.where(f.col('followers_count').isNull()).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eec5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d82adb",
   "metadata": {},
   "source": [
    "We go with: `2. Give them a default value`\n",
    "\n",
    "Give deafult values with - Fill null values - fillna()\n",
    "Give the null cells a default value: Using [fillna](https://nbviewer.org/github/Learn-Apache-Spark/SparkML/blob/master/notebooks/Solution/Solution%201%20-%20Intro%20to%20Data%20Cleaning%20and%20Preparation.ipynb#:~:text=default%20value%3A%20Using-,fillna,-Notice%20the%20matching).\n",
    "\n",
    "Notice the matching type request. Meaning, if a column is of type string, we will need a default value of type string. At the moment, all are fields are of type string.\n",
    "\n",
    "Code sample:\n",
    "\n",
    "```python\n",
    "df_defaultvalue = df.fillna({'column_name':'0'})\n",
    "```\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "<p>\n",
    "    \n",
    "df_defaultvalue = df.fillna({'followers_count':'0'})\n",
    "    \n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Remember to valide yourself with count:\n",
    "\n",
    "```pyhton\n",
    "df_defaultvalue.where(f.col('followers_count').isNull()).count()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da032526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a779c",
   "metadata": {},
   "source": [
    "2nd phase of **standard deviation** calculation is:\n",
    "\n",
    "Casting data to numbers!\n",
    "\n",
    "Cast it to integer:\n",
    "\n",
    "In the code sample, replace the `column_name` with `followers_count`:\n",
    "```pyhton\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "data_df = df_defaultvalue.withColumn(\"column_name\", df_defaultvalue[\"column_name\"].cast(IntegerType()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318c957",
   "metadata": {},
   "source": [
    "### ‚úÖ **Task 7 :**  Gather more statistics to better understand the data\n",
    "\n",
    "Use `pyspark.sql.function` methods, [here are the docs](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).\n",
    "                                                    \n",
    "\n",
    "Check out `describe` functionality. it provides us `count`, `mean`, `stddev`, `min` and `max` calculations in one function!\n",
    "\n",
    "*Remember* - Use the last DataFrame that you created, with the casting and default values.\n",
    "\n",
    "describe can take any field, or calculate statistics for all fields.\n",
    "\n",
    "Code Example:\n",
    "```python\n",
    "df.describe(['age']).show()\n",
    "df.describe().toPandas().transpose()\n",
    "```\n",
    "In the code example, Change age to followers_count and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef098633",
   "metadata": {},
   "source": [
    "### ‚úÖ **Task 8 :**  Continue Marshelling the bot data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe549d",
   "metadata": {},
   "source": [
    "Adapt `bot` column.\n",
    "\n",
    "bot is the data classification column, which indicated if the row represents a bot or not.\n",
    "\n",
    "1. Cast it into Integer.\n",
    "2. Set 1 or 0: 1 for bot and 0 for none bot.\n",
    "If we don't know what it is, use 0.\n",
    "\n",
    "Run the next commands, and remember to validate yourself!\n",
    "Make sure to use the right dataframe.\n",
    "\n",
    "```python\n",
    "df = df.withColumn('bot',df['bot'].cast(IntegerType()))\n",
    "df.limit(5) .toPandas()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now you should have better understanding of the data. let's drop irrelevant columns:\n",
    "\n",
    "Run the next commands:\n",
    "    \n",
    "```python\n",
    "\n",
    "# Dropping irrelevant columns and duplicates\n",
    "df = df.drop('default_profile_image','has_extended_profile','url','created_at','lang','id','id_str')\n",
    "df = dft.dropDuplicates()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7cbc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, ArrayType, BooleanType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d47c41",
   "metadata": {},
   "source": [
    "Next, cast and transform the data into the decided upon column types.\n",
    "\n",
    "Run the next commands and break them down into transformations so it will be easier for you to follow along:\n",
    "\n",
    "```python\n",
    "# First Transformation\n",
    "df_test = df_test.withColumn(\"friends_count\", df_test[\"friends_count\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"listed_count\", df_test[\"listed_count\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"favourites_count\", df_test[\"favourites_count\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"statuses_count\", df_test[\"statuses_count\"].cast(IntegerType()))\n",
    "df_test = df_test.withColumn(\"verified\", df_test[\"verified\"].cast(BooleanType()))\n",
    "df_test = df_test.withColumn(\"default_profile\", df_test[\"default_profile\"].cast(BooleanType()))\n",
    "\n",
    "# Second Transformation\n",
    "df_test = df_test.withColumn('default_profile',df_test['default_profile'].cast(IntegerType()))\n",
    "df_test = df_test.withColumn('name',when(df_test['name'].isNull(),0).otherwise(1))\n",
    "df_test = df_test.withColumn('verified',df_test['verified'].cast(IntegerType()))\n",
    "\n",
    "# Theird Transformation\n",
    "df_test = df_test.withColumn('verified',when(df_test['verified'].isNull(),0).otherwise(df_test['verified']))\n",
    "df_test = df_test.withColumn('default_profile',when(df_test['default_profile'].isNull(),0).otherwise(df_test['default_profile']))\n",
    "df_test = df_test.withColumn('location',when(df_test['location'].isNull(),0).otherwise(1))\n",
    "df_test = df_test.withColumn('status',when(df_test['status'].isNull(),0).otherwise(1))\n",
    "df_test = df_test.withColumn('screen_name',when(df_test['screen_name'].isNull(),0).otherwise(1))\n",
    "\n",
    "# Forth Transformation\n",
    "df_test = df_test.dropna(subset=['description'])\n",
    "\n",
    "def split_and_set(some_str):\n",
    "    if isinstance(some_str, str):\n",
    "        some_str = ''.join(c for c in some_str if c not in \"[](){}<>,'/.\")\n",
    "        return list(set(some_str.split(' ')))\n",
    "    return some_str\n",
    "\n",
    "list_udf = udf(lambda y: split_and_set(y), ArrayType(StringType()))\n",
    "df_test = df_test.withColumn('description', list_udf(df_test['description']))\n",
    "\n",
    "# Fifth Transformation - fill NA:\n",
    "df_test = df_test.fillna({'followers_count':0,'statuses_count':0,'favourites_count':0,'listed_count':0,'friends_count':0,})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14486a",
   "metadata": {},
   "source": [
    "Save this data to parquest so it is easier to work with later\n",
    "\n",
    "```python\n",
    "df_test.write.parquet(\"marshalled_data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8150e7",
   "metadata": {},
   "source": [
    "#### Well Done! üëèüëèüëè\n",
    "You just finished: Marshaling the data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
